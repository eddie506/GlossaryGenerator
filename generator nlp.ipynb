{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossary Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import textract\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "def remove_punctuation(from_text):\n",
    "    # map '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' to ''\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in from_text]\n",
    "    return stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file into a list of sentences\n",
    "filename = 'Stories of Your Life and Others by Ted Chiang.txt'\n",
    "byte = textract.process(filename)\n",
    "text = byte.decode(\"utf-8\")\n",
    "tokenized_text = sent_tokenize(text)\n",
    "tokens = [[word for word in line.split()] for line in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en_core_web_sm' model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract lemmas\n",
    "lemmas = []\n",
    "for s in tokens:\n",
    "    doc = nlp(' '.join(s))\n",
    "    lemmas.append(remove_punctuation([token.lemma_ for token in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords such as 'the', 'i'\n",
    "sw = (stopwords.words('english'))\n",
    "words = [token for sentence in lemmas for token in sentence if (token.lower() not in sw and token.isalnum())]\n",
    "\n",
    "word_count = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# favors less frequent words (??\n",
    "sorted_words = sorted(word_count, key=word_count.get, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard the first 10000 words in the dictionary. \n",
    "# this number should be adjusted according to the user's own ability\n",
    "discard = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('common30k.txt') as dict:\n",
    "    dict_words = [word for line in dict.readlines() for word in line.split()]\n",
    "    less_common_dict_words = dict_words[discard + 1:]\n",
    "    new_words = [word for word in sorted_words if word in less_common_dict_words]\n",
    "    print(\"There are\", len(new_words), \"potential new words\")    \n",
    "    glossary = filename.split('.')[0] + '_glossary.txt'\n",
    "    with open(glossary, 'w') as output:\n",
    "        output.write('\\n'.join(new_words))\n",
    "        print(\"Wrote to\", glossary)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9894c183e24c00ceac7d354d9380ff1faf7bd876c4888c5b820da59dd19580d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
